\chapter{Markov Chain Monte Carlo Methods}\label{ch-mcmc}
\thispagestyle{headings}
\markboth{Chapter \ref{ch-mcmc}: Markov Chain Monte Carlo Methods}{Chapter \ref{ch-mcmc}: Markov Chain Monte Carlo Methods}

The PECOS Toolkit currently implements the DRAM algorithm \cite{HaLaMiSa06} for the generation of a Markov chain.
After a review of some concepts and results on Markov chains in Section \ref{sc-mcmc-markov-chains},
Section \ref{sc-mcmc-realization-of-a-markov-chain} details the
Metropolis-Hastings (MH),
Delayed Rejection (DR),
Adaptive Metropolis (AM) and
DRAM algorithms.
Section \ref{sc-mcmc-how-to-use-dram} explains how to to develop your own application using the DRAM capabilities of the PECOS Toolkit, while
Section \ref{sc-mcmc-dram-examples} describes the examples available in the toolkit.
The chapter ends at Section \ref{sc-mcmc-planned-features} with a brief list of planned features for next toolkit versions w.r.t. Markov Chain Monte Carlo methods.

\section{Markov Chains}\label{sc-mcmc-markov-chains}

This section 
is based primarily on references \cite{Du05} and \cite{JaPr04}.
The subject of some subsections can be found at \cite{KaSo05} and \cite{Ro96} as well.

\subsection{Basic Concepts}

Let $(\Omega,U,P)$ be a probability space and $Y:\Omega\rightarrow S$ be a $U$-measurable r.v.,
that is, a measurable map relative to the $\sigma$-algebras $U$ and $\mathcal{S}$,
as explained in Subsection \ref{subsc-intro-prelim-basic}, page \pageref{subsc-intro-prelim-basic}.

Now let $Y:\Omega\rightarrow\mathbb{R}$ be a $U$-measurable scalar r.v..
We will assume that the reader already knows the concept of the
integration of $Y$
w.r.t. a probability measure $M:\mathbb{R}\rightarrow [0,1]$
over a Borel set $B\in\mathfrak{B}(\mathbb{R})$ (see, e.g., \cite[Section A.4]{Du05} and \cite[Chapter 9]{JaPr04}).
When such integral exists (it might assume values $\pm\infty$), it will be denoted by
\begin{equation*}
\int_B y~dM(y).
\end{equation*}
When $M=P_Y$, the probability distribution of $Y$, we use the following notations:
\begin{equation*}
\int_B y~dP_Y(y) = 
\int_{Y^{-1}(B)}Y(\omega)dP(\omega).
\end{equation*}

Given a r.v. $\mathbf{Y}:\Omega\rightarrow\mathbb{R}^n$ and any arbitrarily fixed $B\in\mathfrak{B}(\mathbb{R}^n)$,
we define the r.v. 
\begin{equation*}
\mathbf{1}_{\mathbf{Y}\in B}:\Omega\rightarrow\{0,1\},
~\mathbf{1}_{\mathbf{Y}\in B}(\omega)=1\mbox{ if }\mathbf{Y}(\omega)\in B,
~\mathbf{1}_{\mathbf{Y}\in B}(\omega)=0\mbox{ otherwise}.
\end{equation*}

For a vector r.v. $Y:\Omega\rightarrow\mathbb{R}^n$ and $B\in\mathfrak{B}(\mathbb{R}^n)$,
the concepts presented so far are just applied componentwise.

\subsection{Expectations}

Let $\mathbf{Y}:\Omega\rightarrow \mathbb{R}^n$ be a $U$-measurable r.v. on the probability space $(\Omega,U,P)$.

The {\it expectation} $E_P[\mathbf{Y};A]\in\mathbb{R}^n$
of $\mathbf{Y}$
over an event $A\in U$
w.r.t. the probability measure $P:\Omega\rightarrow [0,1]$
is defined by
\begin{equation*}
E_P[\mathbf{Y};A] \equiv
\int_{A} \mathbf{Y}(\omega)~dP(\omega),
\end{equation*}
when such integral exists, including values $\pm\infty$.
We might simply write $E[\mathbf{Y};A]$.
When $A=\Omega$ we might simply write $E_P[\mathbf{Y}]$ or $E[\mathbf{Y}]$.

When appropriate for the clarification of expressions
we might eventually denote expectations with brakets $\{\cdot\}$ instead of using the $[\cdot]$ notation.

The {\it expectation} $E_M[\mathbf{Y};B]\in\mathbb{R}^n$
of $\mathbf{Y}$
over a Borel set $B\in\mathfrak{B}(\mathbb{R}^n)$
w.r.t. a probability measure $M:\mathbb{R}^n\rightarrow [0,1]$
is defined by
\begin{equation*}
E_M[\mathbf{Y};B] \equiv
\int_{B} \mathbf{y}~dM(\mathbf{y})
\end{equation*}
when such integral exists, including values $\pm\infty$.
When $B=\mathbb{R}^n$ we simply write
$E_M[\mathbf{Y}]$.

When $M=P_{\mathbf{Y}}$, the probability distribution of $\mathbf{Y}$, we write
\begin{equation*}
E_{P_{\mathbf{Y}}}[\mathbf{Y};B] \equiv
\int_{B} \mathbf{y}~dP_\mathbf{Y}(\mathbf{y}) =
\int_{\mathbf{Y}^{-1}(B)}\mathbf{Y}(\omega)dP(\omega) \equiv
E_P[\mathbf{Y};\mathbf{Y}^{-1}(B)]
\end{equation*}
or simply
$E[\mathbf{Y};B]$.

If $M=P_{\mathbf{Y}}$ and $B=\mathbb{R}^n$, then we use the following notations:
\begin{equation*}
E_{P_{\mathbf{Y}}}[\mathbf{Y}] \equiv
\int_{\mathbb{R}^n} \mathbf{y}~dP_\mathbf{Y}(\mathbf{y}) = 
\int_{\Omega}\mathbf{Y}(\omega)dP(\omega) \equiv
E_P[\mathbf{Y}],
\end{equation*}
or simply
$E[\mathbf{Y}]$.

\subsection{Integrable R.V.'s}

A r.v. $\mathbf{Y}$ is called {\it integrable} (w.r.t. the probability measure $P_{\mathbf{Y}}$) if and only if its expectation exists and is finite.
We will write $\mathcal{L}^1(\Omega,U,P)$, sometimes just $\mathcal{L}^1$ for short, to denote the vector space of all integrable random variables.
It is easy to check that:
\[
\begin{array}{rl}
(i)  & Y\in\mathcal{L}^1\mbox{ iff }|Y|\in\mathcal{L}^1,\mbox{ and}   \\
(ii) & Y_1=Y_2\mbox{ almost surely (a.s.) }\Rightarrow~E[Y_1]=E[Y_2]. \\
\end{array}
\]
For $1 < p < \infty$ we define $\mathcal{L}^p(\Omega,U,P)$ to be the space of r.v.'s $Y$ such that $|Y|^p\in\mathcal{L}^1$.

Since the a.s. equality is an equivalence relation between two r.v.'s,
we define $L^p(\Omega,U,P)$, $1\leqslant p < \infty$,
to be $\mathcal{L}^p$ module the ``a.s. equality'' equivalence relation,
and we might denote it $L^p$ for short. That is,
two elements of $\mathcal{L}^p$ that are a.s. equal are considered to be
representatives (``versions'') of the same element in $L^p$.

The space $L^2(\Omega,U,P)$ is a Hilbert space with inner product
\begin{equation*}
\langle \mathbf{Y},\mathbf{Z} \rangle = E[\mathbf{Y}\mathbf{Z}].
\end{equation*}
The space $L^2(\Omega,\sigma(\mathbf{Y}),P)$ is a (closed) Hilbert subspace of $L^2(\Omega,U,P)$.
The space $L^1(\Omega,\mathbf{Y},P)$ is a Banach space.
% with norm

For the case of a generic r.v. $\mathbf{Y}:\Omega\rightarrow S$,
let $h:S\rightarrow\mathbb{R}$ be a measurable map relative to $\mathcal{S}$ and $\mathfrak{B}(\mathbb{R})$, that is,
a r.v. on $(S,\mathcal{S},P_{\mathbf{Y}})$. See Figure \ref{fig-mcmc-hY-diagram}. One can then prove that \cite{JaPr04}
\begin{equation*}
h(\mathbf{Y})\in\mathcal{L}^1(\Omega,U,P)\mbox{ iff }h\in\mathcal{L}^1(S,\mathcal{S},P_{\mathbf{Y}}),
\end{equation*}
and
\begin{equation*}
h\mbox{ is positive or }h\in\mathcal{L}^1(S,\mathcal{S},P_{\mathbf{Y}})
\Rightarrow
E[h(\mathbf{Y})] = \int_S h(\mathbf{Y})P_{\mathbf{Y}}(d\mathbf{y}).
\end{equation*}
Moreover, if $S=\mathbb{R}^n$ then
\begin{equation*}
\mathbf{Y}\mbox{ has a probability density }\pi_{\mathbf{Y}}:\mathbb{R}^n\rightarrow [0,1]
\Rightarrow
E[h(\mathbf{Y})] = \int_{\mathbb{R}^n}h(\mathbf{y})\pi_{\mathbf{Y}}(\mathbf{y})~d\mathbf{y}.
\end{equation*}

\begin{figure}[h]
\[
\begin{CD}
\Omega   @>Y>> S           @>h>> \mathbb{R}              \\
\in      @.    \in         @.    \in                     \\
U        @.    \mathcal{S} @.    \mathfrak{B}(\mathbb{R})\\
@VP VV         @VVP_YV           @VVP_hV                 \\
[0,1]    @.    [0,1]       @.    [0,1]
\end{CD}
\]
\caption{The composition of a measurable map $h$ with a r.v. $\mathbf{Y}$,
resulting on a new r.v. $h(\mathbf{Y})$.
}
\label{fig-mcmc-hY-diagram}
\end{figure}

\subsection{Conditional Expectation}

Let $(\Omega,U,P)$ be a probability space,
$\mathcal{F}$ be a sub-$\sigma$-algebra of $U$,
and $Y:\Omega\rightarrow\mathbb{R}^n$ be an integrable r.v., that is, $\mathbf{Y}\in L^1(\Omega,U,P)$.
Note that we are {\it not} assuming that $Y$ is $\mathcal{F}$-measurable.
The {\it conditional expectation} of $\mathbf{Y}$ given $\mathcal{F}$,
denoted $E[\mathbf{Y}|\mathcal{F}]$,
is defined \cite{JaPr04} as the unique r.v. $E[\mathbf{Y}|\mathcal{F}]=\mathbf{X}:\Omega\rightarrow\mathbb{R}^n$ satisfying
\begin{equation}\label{eq-l1-cond-expec-wrt-algebra-A}
\mathbf{X}\in L^1(\Omega,\mathcal{F},P)
\end{equation}
and
\begin{equation}\label{eq-l1-cond-expec-wrt-algebra-B}
E_P[\mathbf{Y}\mathbf{Z};\Omega]\equiv E[\mathbf{Y}\mathbf{Z}] = E[\mathbf{X}\mathbf{Z}] \equiv E_P[\mathbf{X}\mathbf{Z};\Omega]
\quad
\forall\mbox{ bounded }\mathcal{F}\mbox{-measurable }\mathbf{Z}:\Omega\rightarrow\mathbb{R}^n.
\end{equation}
An alternative definition \cite{Du05} replaces \eqref{eq-l1-cond-expec-wrt-algebra-B} by
\begin{equation*}
E_P[\mathbf{Y};A]\equiv \int_A \mathbf{Y}(\omega)~dP(\omega) = \int_A \mathbf{X}(\omega)~dP(\omega)\equiv E_P[\mathbf{X};A] \quad\forall A\in\mathcal{F}.
\end{equation*}
For the case of $\mathbf{Y}\in L^2(\Omega,U,P)$ it can be shown \cite{JaPr04} that the
unique r.v. $\mathbf{X}:\Omega\rightarrow\mathbb{R}^n$ defined by \eqref{eq-l1-cond-expec-wrt-algebra-A}-\eqref{eq-l1-cond-expec-wrt-algebra-B} satisfies
\begin{equation}\label{eq-l2-cond-expec-wrt-algebra-A}
\mathbf{X}\in L^2(\Omega,\mathcal{F},P)
\end{equation}
and
\begin{equation}\label{eq-l2-cond-expec-wrt-algebra-B}
E[\mathbf{Y}\mathbf{Z}] = E[\mathbf{X}\mathbf{Z}]
\quad
\mbox{for all }\mathbf{Z}\in L^2(\Omega,\mathcal{F},P),
\end{equation}
that is, $\mathbf{X}$ can be interpreted as the projection of $\mathbf{Y}$ into the subspace $L^2(\Omega,\mathcal{F},P)$.

We define the conditional expectation $E[Y|X]$ of a r.v. $Y$ w.r.t. another r.v. $X$ by
\begin{equation*}
E[Y|X] = E[Y|\sigma(X)].
\end{equation*}

Given $\mathbf{Y}\in L^1(\Omega,U,P)$ and
a sub-$\sigma$-algebra $\mathcal{F}$,
we define,
for any given $B\in\mathfrak{B}(\mathbb{R}^n)$,
\begin{equation}\label{eq-cond-prob-wrt-algebra}
P_{\mathbf{Y}}(B|\mathcal{F}) = E[\mathbf{1}_{\mathbf{Y}\in B}|\mathcal{F}].
\end{equation}

Some properties of conditional expectations are:
\begin{equation}\label{eq-cond-expec-know-Y}
Y\mbox{ is }\mathcal{F}\mbox{-measurable}
~\Rightarrow~
E[Y|\mathcal{F}]=Y,
\end{equation}
\begin{equation}\label{eq-cond-expec-know-nothing}
Y\mbox{ and }\mathcal{F}\mbox{ are independent (see }\eqref{eq-Y-F-independent}\mbox{)}
~\Rightarrow~
E[Y|\mathcal{F}]=E[Y],
\end{equation}
\begin{equation}\label{eq-cond-expec-independent-rvs}
Y\mbox{ and }X\mbox{ are independent r.v.'s}
~\Rightarrow~
E[Y|X]=E[Y],
\end{equation}
\begin{equation}\label{eq-cond-expec-smaller-wins}
\mathcal{F}_1\subset\mathcal{F}_2
~\Rightarrow~
E\{E[Y|\mathcal{F}_2]|\mathcal{F}_1\} = E[Y|\mathcal{F}_1]\mbox{ and }E\{E[Y|\mathcal{F}_1]|\mathcal{F}_2\} = E[Y|\mathcal{F}_1],
\end{equation}
\begin{equation}\label{eq-cond-expec-f-u}
\mathcal{F}\subset U\mbox{ and }E[Y|U]\mbox{ is }\mathcal{F}\mbox{-measurable}
~\Rightarrow~
E[Y|\mathcal{F}] = E[Y|U].
\end{equation}
and
\begin{equation}\label{eq-cond-expec-exists-g}
\left\{
\begin{array}{c}
\mathcal{F}=\sigma(X)\mbox{ for some r.v. }X:\Omega\rightarrow\mathbb{R}^m \\
~\Rightarrow~ \\
\exists~\mathfrak{B}(\mathbb{R}^m)\mbox{-measurable }g:\mathbb{R}^m\rightarrow\mathbb{R}^n\mbox{ such that }E[Y|\mathcal{F}] = g(X).
\end{array}
\right.
\end{equation}

We can interpret the properties just listed by grasping the intuitive idea of a $\sigma$-algebra $\mathcal{F}$ in the context of r.v.'s.
Let us say that the outcome $\omega$ of a performed random experiment is unknown. Instead, for each $A\in\mathcal{F}$ we are told if $\omega\in A$.
In this scenario, more sets in $\mathcal{F}$ means more information to us for determining $\omega$ and might mean more information to us for determining the value $\mathbf{Y}(\omega)$ of a r.v..
Result \eqref{eq-cond-expec-know-Y}, for instance, establishes that if $\mathbf{Y}$ is $\mathcal{F}$-measurable, the information in $\mathcal{F}$ is enough to determine the value $\mathbf{Y}(\omega)$,
although the information might not be enough to determine $\omega$ itself.
On the other extreme, result \eqref{eq-cond-expec-know-nothing} establishes that an independent $\sigma$-algebra is useless for the determination of $\mathbf{Y}(\omega)$.
As a particular case, the trivial $\sigma$-algebra provides no information.
Result $\eqref{eq-cond-expec-smaller-wins}$ establishes that the smaller $\sigma$-algebras ``wins''.

\subsection{Transition Probability}

Given a function $Q:S\times\mathcal{S}\rightarrow [0,1]$, let us define
\begin{equation*}
\mbox{for any }y\in S,\mbox{ the mapping }Q_y:\mathcal{S}\rightarrow [0,1]\mbox{ by }Q_y(B)=Q(y,B),
\end{equation*}
and
\begin{equation*}
\mbox{for any }B\in\mathcal{S},\mbox{ the mapping }Q_B:S\rightarrow [0,1]\mbox{ by }Q_B(y)=Q(y,B).
\end{equation*}
The function $Q:S\times\mathcal{S}\rightarrow [0,1]$ is then said to be a {\it transition probability} if
\begin{equation*}
\mbox{for each }y\in S,\mbox{ the mapping }Q_y:\mathcal{S}\rightarrow [0,1]\mbox{ is a probability measure on }(S,\mathcal{S})
\end{equation*}
and
\begin{equation*}
\mbox{for each }B\in\mathcal{S},\mbox{ the mapping }Q_B:S\rightarrow [0,1]\mbox{ is a measurable function}.
\end{equation*}
Intuitively, the value $Q(y,B)$ can be understood as
the probability that a random variable will transit/change from the current value $y$ to a value in the region $B$.
It is a concept related to Markov Chains, as we will see in the next section.

\subsection{Filtration, Adapted Sequence of R.V.'s and Markov Chains}

Let $(\Omega,U,P)$ be a probability space and
let $\{\mathbf{Y}_k\}_{k\geqslant 0}$ be a sequence of r.v.'s
respectively defined over the probability spaces $(\Omega,U_k,P)$ and
taking values on the same measurable space $(S,\mathcal{S})$.
We refer to $S$ as the state space.

A sequence $\{\mathcal{F}_k\}$ of increasing $\sigma$-algebras over $\Omega$ is called a {\it filtration}.
We say that $\{\mathbf{Y}_k\}$ is {\it adapted} to the filtration $\{\mathcal{F}_k\}$ if $\mathbf{Y}_k$ is $\mathcal{F}_k$-measurable for all $k$.
From now on we will assume $U=U_k$ and $\mathcal{F}_k\subseteq U$ for all $k$.

By definition, a sequence $\{\mathbf{Y}_k\}$ of r.v.'s is said to be a Markov Chain, or to possess the Markov property, w.r.t. a filtration $\{\mathcal{F}_k\}$,
if $\{\mathbf{Y}_k\}$ is adapted to $\{\mathcal{F}_k\}$ and
\begin{equation}\label{eq-markov-chain-general-def-1}
P_{\mathbf{Y}_{k+1}}(B|\mathcal{F}_k) = P_{\mathbf{Y}_{k+1}}(B|\sigma(\mathbf{Y}_k))\quad\forall B\in\mathcal{S}.
\end{equation}
It should be noted that, according to $\eqref{eq-cond-prob-wrt-algebra}$, $\eqref{eq-markov-chain-general-def-1}$ is an equality between r.v.'s.
Intuitively, it establishes that given the present, represented by $\sigma(\mathbf{Y}_k)$,
the rest of the past is irrelevant for predicting $\mathbf{Y}_{k+1}$.

It can be proved \cite{Du05} that $\{\mathbf{Y}_k\}$ is a Markov Chain w.r.t. a filtration $\{\mathcal{F}_k\}$ iff
there exist transition probabilities $Q_k:S\times\mathcal{S}\rightarrow [0,1]$ such that
\begin{equation}\label{eq-markov-chain-general-def-2}
P_{\mathbf{Y}_{k+1}}(B|\mathcal{F}_k) = \mathbf{Q}_k(\mathbf{Y}_k,B)\quad\forall B\in\mathcal{S},
\end{equation}
where, for any arbitrarily fixed $B\in\mathcal{S}$, $\mathbf{Q}_k(\mathbf{Y}_k,B):\Omega\rightarrow [0,1]$ is the r.v. defined by
\begin{equation*}
\left[\mathbf{Q}_k(\mathbf{Y}_k,B)\right](\omega) = Q_k(\mathbf{Y}_k(\omega),B).
\end{equation*}

If $S$ is countable, then \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-countable-def-1}
P_{\mathbf{Y}_{k+1}}(
\{\mathbf{y}_{k+1}\}
|
\{\mathbf{y}_k\},
\ldots,
\{\mathbf{y}_0\}
)
=
P_{\mathbf{Y}_{k+1}}(
\{\mathbf{y}_{k+1}\}
|
\{\mathbf{y}_k\}
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1,
\end{equation}
and
\begin{equation}\label{eq-markov-chain-countable-def-2}
P_{\mathbf{Y}_{k+1}}(
\{\mathbf{y}_{k+1}\}
|
\{\mathbf{y}_k\},
\ldots,
\{\mathbf{y}_0\}
)
=
Q_k(
\mathbf{y}_k,
\mathbf{y}_{k+1}
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1,
\end{equation}
where
$P_{\mathbf{Y}_{k+1}}(\cdot|\ldots)$ now means a real value on both equations,
instead of a r.v. as in \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2}.

If $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$, then \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-R-def-1}
P_{\mathbf{Y}_{k+1}}(
B_{k+1}
|
B_k,
\ldots,
B_0
)
=
P_{\mathbf{Y}_{k+1}}(
B_{k+1}
|
B_k
)
\quad\forall
B_i\in\mathcal{S},~0\leqslant i\leqslant k+1,
\end{equation}
and
\begin{equation}\label{eq-markov-chain-R-def-2}
P_{\mathbf{Y}_{k+1}}(
B_{k+1}|
B_k
\ldots,
B_0
)
=
\int_{B_k} Q_k(
\mathbf{y},B_{k+1}
)~d\mathbf{y}
\quad\forall
B_i\in\mathcal{S},~0\leqslant i\leqslant k+1,
\end{equation}
where, again,
$P_{\mathbf{Y}_{k+1}}(\cdot|\ldots)$ now means a real value on both equations,
instead of a r.v. as in \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2}.

If $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$ and
the r.v.'s are absolutely continuous, i.e., their probability distributions can be expressed in terms of probability densities,
then \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-Rac-def-1}
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k,
\ldots,
\mathbf{y}_0
)
=
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1,
\end{equation}
and
\begin{equation}\label{eq-markov-chain-Rac-def-2}
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k,
\ldots,
\mathbf{y}_0
)~d\mathbf{y}
=
dQ_k(
\mathbf{y}_k,
\mathbf{y}_{k+1},
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1,
\end{equation}
where, once more,
$P_{\mathbf{Y}_{k+1}}(\cdot|\ldots)$ now means a real value on both equations,
instead of a r.v. as in \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2}.

\subsection{Transition Probability Kernel}

For the case $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$,
a function $K:S\times S\rightarrow [0,1]$ is called the kernel of
a transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$
if $Q$ can be expressed by
\begin{equation*}
Q(x,B) = \int_B {K}(x,y)~dy \quad\forall (x,B)\in S\times\mathcal{S}.
\end{equation*}
Clearly, a transition probability kernel needs to satisfy
\begin{equation*}
\int_S {K}(x,y)~dy=1.
\end{equation*}

So, going back to the last case analysed in the last subsection,
if $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$ and
the r.v.'s are absolutely continuous,
then
\eqref{eq-markov-chain-Rac-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-Rac-def-3}
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k,
\ldots,
\mathbf{y}_0
)
=
K_k(
\mathbf{y}_k,
\mathbf{y}_{k+1},
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1.
\end{equation}

\subsection{Homogeneous Markov Chains}

A Markov Chain is said to be {\it homogeneous} if the transition probabilities do not depend on $k$.
From now on, unless stated otherwise,
we will assume that a Markov Chain is homogeneous with transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$.

In this subsection we will give a simple example exploring the concepts of transition probability kernel and homogeneity.
Let us assume a finite state space with $N>0$ possible elements.
Without loss of generality, we can define $S$ to be the set $\{1,2,\ldots,N\}$ and $\mathcal{S}$ to be the $\sigma$-algebra formed by all subsets of $S$.
Suppose also we are given:
{\renewcommand{\labelitemi}{}
\begin{itemize}
\item $(i)$ a r.v. $Y_0$ with known probability distribution $P_{Y_0}:\mathcal{S}\rightarrow [0,1]$ and
\item {$(ii)$ quantities ${K}_{ij}\geqslant 0$, $(i,j)\in S\times S$, satisfying
\begin{equation*}
\sum_{j=1}^{N}{K}_{ij} = 1.
\end{equation*}
}
\end{itemize}
}
Intuitively, the quantity ${K}_{ij}$ can be thought as the probability of going from state ``$i$'' to state ``$j$''.
Let us denote by ``$K$'' the $N\times N$ matrix whose element at position $(i,j)$ has value ${K}_ij$.

Now, with $(i)$ and $(ii)$, let us construct a chain $\{Y_0,Y_1,\ldots\}$ as follows.
First, let us define the function $Q:S\times\mathcal{S}\rightarrow [0,1]$ by
\begin{equation*}
Q(i,B) = \sum_{j\in B}{K}_{ij}.
\end{equation*}
Clearly, $Q$ is a transition probability.
Second, for $k\geqslant 0$, let us assume that the probability $P_{Y_{k}}$ is known and
let us recursevily define the r.v. $Y_{k+1}:\Omega\rightarrow [0,1]$,
not immediately
by admitting the knowledge of its probability distribution $P_{Y_{k+1}}:S\rightarrow [0,1]$,
but instead
by setting the conditional probability of $Y_{k+1}$ w.r.t. to each possible value of $Y_k$:
\begin{equation*}
P_{Y_{k+1}|\{i\}}(\{j\}|\{i\})={K}_{ij}.
\end{equation*}
Let us then find an explicit formula for
\begin{equation*}
P_{Y_{k+1}} = P_{mY_{k+1}},
\end{equation*}
the term on the right meaning the marginal distribution of $Y_{k+1}$ when thinking in terms of a joint probability distribution between $Y_{k+1}$ and $Y_k$.
For easiness of notation, we will simply write
\begin{equation*}
P_{Y_{k+1}|i}(j|i)={K}_{ij}.
\end{equation*}
%
By remembering that
\begin{equation*}
P_{Y_{k+1}|i}(j|i) = \frac{P_{Y_{k+1}Y_k}(j,i)}{P_{mY_k}(i)} = \frac{P_{Y_{k+1}Y_k}(j,i)}{P_{Y_k}(i)}
\end{equation*}
and
\begin{equation*}
P_{Y_{k+1}}(j) = P_{mY_{k+1}}(j) = P_{Y_{k+1}Y_k}(j,S) = \sum_{i=1}^{N}P_{Y_{k+1}Y_k}(j,i),
\end{equation*}
we conclude
\begin{equation*}
P_{Y_{k+1}}(j) = \sum_{i=1}^{N}P_{Y_k}(i){K}_{ij}.
\end{equation*}
Clearly, $P_{Y_{k+1}}$ is a measure, since
\begin{equation*}
\sum_{j=1}^{N}P_{Y_{k+1}}(j) = \sum_{j=1}^{N}\sum_{i=1}^{N}P_{Y_k}(i){K}_{ij} = \sum_{i=1}^{N}P_{Y_k}(i)\sum_{j=1}^{N}{K}_{ij} = \sum_{i=1}^{N}P_{Y_k}(i) = 1.
\end{equation*}
If we represent $P_{Y_k}$ as a row vector, then
\begin{equation}\label{eq-markov-recursive-relation-finite}
P_{Y_{k+1}}  = P_{Y_k}K,\quad k\geqslant 0,
\end{equation}
and
\begin{equation*}
P_{Y_k}  = P_{Y_0}{K}^k,\quad k\geqslant 0.
\end{equation*}

It can be proved that the chain
\begin{equation*}
\{Y_0,Y_1,\ldots\}
\end{equation*}
constructed in this way is a homogeneous Markov Chain
w.r.t. the filtration $\{\mathcal{F}_{k+1}\}$ with $\mathcal{F}_{k+1}=\sigma(Y_0,Y_1,\ldots,Y_{k+1})$ and
with transition probability kernerl given by the matrix $K$.

For any general Markov Chain, the probability measure $P_{X_0}$ is called the {\it initial distribution} of the chain.

For the case $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n)$, given
{\renewcommand{\labelitemi}{}
\begin{itemize}
\item $(i)$ a r.v. $Y_0$ with known probability density $\pi_{Y_0}:\mathcal{S}\rightarrow [0,1]$ and
\item $(ii)$ a transition probability kernel $K:S\times S\rightarrow [0,1]$,
\end{itemize}
}
we can similarly construct a Markov Chain as done for the case of a finite state space.
Equation \eqref{eq-markov-recursive-relation-finite} now becomes
\begin{equation}\label{eq-markov-recursive-relation-Rac}
\pi_{Y_{k+1}}(z) = \int_{S}\pi_{Y_k}(y)~K(y,z)~dy,\quad\forall z\in S,~k\geqslant 0.
\end{equation}

For the more general case of $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n)$, given
{\renewcommand{\labelitemi}{}
\begin{itemize}
\item $(i)$ a r.v. $Y_0$ with known probability distribution $P_{Y_0}:\mathcal{S}\rightarrow [0,1]$ and
\item $(ii)$ a transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$,
\end{itemize}
}
we can also similarly construct a Markov Chain, with
Equation \eqref{eq-markov-recursive-relation-finite} now becoming
\begin{equation}\label{eq-markov-recursive-relation-R}
P_{Y_{k+1}}(B) = \int_{y\in S}dP_{Y_k}(y)~Q(y,B),\quad\forall B\in\mathcal{S},~k\geqslant 0.
\end{equation}

\subsection{Stationary Distributions and Irreducible Aperiodic Chains}

A probability distribution $M:S\rightarrow [0,1]$ is said to be stationary w.r.t. a transition probability $Q$ if
\begin{equation*}
M(B) = \int_{y\in S}dM(y)~Q(y,B),\quad\forall B\in\mathcal{S}.
\end{equation*}

A probability density $\pi_M:S\rightarrow [0,1]$ is said to be stationary w.r.t. a transition probability kernel $K$ if
\begin{equation}\label{eq-stationary-density}
\pi_M(z) = \int_{S}\pi_M(y)~K(y,z)~dy,\quad\forall z\in S.
\end{equation}

For the case of finite state spaces, stationarity for a distribution means
\begin{equation*}
M = MK.
\end{equation*}

\subsection{Limit Density of a Transition Probabiliy Kernel}
$~$\\

\subsection{Ergodic Sequences}

To be explained in future versions of the documentation.

\subsection{Variance, Covariance and Covariance Matrix}

Although the concepts of variance and covariance are very important, we delayed their definitions in order to make it clear that Markov chains involve the concepts of probability distributions and expectation primarily.
Covariance matrices will be explicitly mentioned during the presentation of the Adaptive Metropolis algorithm in Section \ref{subsc-mcmc-am-alg}.

\section{The Realization of A Markov Chain}\label{sc-mcmc-realization-of-a-markov-chain}

In this Section \ref{sc-mcmc-realization-of-a-markov-chain}
we will use upper indices to indicate r.v.'s and their realizations, e.g. $y^{(k)}$ and $Y^{(k)}$.

In what follows we also assume that
we are given
\begin{equation}\label{eq-pi-target}
\mbox{a target probability density }\pi_{\mbox{target}}:\mathbf{R}^n\rightarrow [0,1],
\end{equation}
\begin{equation}\label{eq-y0}
\mbox{a relization }y^{(0)}\mbox{ of a r.v. }Y^{(0)}:\Omega\rightarrow\mathbb{R}^n,
\end{equation}
\begin{equation}\label{eq-q1}
\mbox{a proposal kernel (see Subsection \ref{subsc-proposal-kernel}) }q_1:\mathbb{R}^n\times \mathbb{R}^n\rightarrow [0,1]\mbox{ and}
\end{equation}
\begin{equation}\label{eq-n-chain}
\mbox{an integer }N_{\mbox{chain}}\geqslant 1,
\end{equation}
and that
we want to construct a {\it realization} of
a homogeneous Markov chain of size $N_{\mbox{chain}}$ with a transition probability kernel $K:\mathbb{R}^n\rightarrow [0,1]$
such that the limit density of $K$ is $\pi_{\mbox{target}}$.
An example of a target is the density equal (up to a multiplicative constant) to the posterior density discussed
at the end of Section \ref{sc-intro-qoi}.

\subsection{The Proposal Transition Probability Kernel $q$}\label{subsc-proposal-kernel}
$~$\\

\subsection{The Function $\alpha$}

Before we proceed to the actual algorithms, a definition is necessary.
Given a probability density $\pi:\mathbb{R}^n\rightarrow [0,1]$,
an integer $k\geqslant 1$ and
$k$ proposal transition probability kernels
\begin{equation*}
q_i:\underbrace{\mathbb{R}^n\times\ldots\times\mathbb{R}^n}_{(i+1)\mbox{ times}}\rightarrow [0,1],\quad 1\leqslant i\leqslant k,
\end{equation*}
we recursively define
\begin{equation}\label{eq-alphas}
\alpha_i:\underbrace{\mathbb{R}^n\times\ldots\times\mathbb{R}^n}_{(i+1)\mbox{ times}}\rightarrow [0,1],\quad 1\leqslant i\leqslant k,
\end{equation}
by setting
\begin{equation*}
\alpha_1(y,c^{(1)}) = \mbox{ min}
\left\{
1,\frac
{\pi(c^{(1)})q_1(c^{(1)},y)}
{\pi(y)q_1(y,c^{(1)})}
\right\},
\end{equation*}
and, for $i>1$,
\begin{equation*}
\alpha_i(y,c^{(1)},\ldots,c^{(i)}) = \mbox{ min}
\left\{
1,\frac
{\pi(c^{(i)})}
{\pi(y)}
\cdot q_{\mbox{fraction}}
\cdot \alpha_{\mbox{fraction}}
\right\}.
\end{equation*}
where
the expressions $q_{\mbox{fraction}}$ and $\alpha_{\mbox{fraction}}$ are given by
\begin{equation*}
q_{\mbox{fraction}}=
\frac
{q_1(c^{(i)},c^{(i-1)})}
{q_1(y,c^{(1)})}
\frac
{q_2(c^{(i)},c^{(i-1)},c^{(i-2)})}
{q_2(y,c^{(1)},c^{(2)})}
\ldots
\frac
{q_i(c^{(i)},c^{(i-1)},\ldots,c^{(1)},y)}
{q_i(y,c^{(1)},\ldots,c^{(i-1)},c^{(i)})}
\end{equation*}
and
\begin{equation*}
\alpha_{\mbox{fraction}}=
\frac
{[1-\alpha_1(c^{(i)},c^{(i-1)})]}
{[1-\alpha_1(y,c^{(1)})]}
\frac
{[1-\alpha_2(c^{(i)},c^{(i-1)},c^{(i-2)})]}
{[1-\alpha_2(y,c^{(1)},c^{(2)})]}
\ldots
\frac
{[1-\alpha_{i-1}(c^{(i)},c^{(i-1)},\ldots,c^{(1)})]}
{[1-\alpha_{i-1}(y,c^{(1)},\ldots,c^{(i-1)})]}.
\end{equation*}
It should be emphasized that $y$ does {\it not} appear on the numerator of $\alpha_{\mbox{fraction}}$.

\subsection{The Metropolis-Hastings (MH) Algorithm}%\label{subsc-mcmc-mh-alg}

The Metropolis-Hastings algorithm proceeds as follows:
\begin{enumerate}
\item for ($k=0$; $k < (N_{\mbox{chain}}-1)$; ++$k$) \{
\item $\quad$/* Perform $(k+1)$-th iteration in order to obtain $y^{(k+1)}$ */
\item $\quad$generate a candidate $c$ from $q_1(y^{(k)},\cdot)$;
\item $\quad$compute the acceptance ratio $\alpha=\alpha_1(y^{(k)},c)$ (see \eqref{eq-alphas});
\item $\quad$draw $t$ from the uniform distribution on $[0,1]$;
\item $\quad$if ($t\leqslant \alpha$) $y^{(k+1)}=c$;
\item $\quad$else $y^{(k+1)}=y^{(k)}$;
\item \} /* end for */.
\end{enumerate}

\subsection{The Delayed Rejeciton (DR) Algorithm}%\label{subsc-mcmc-dr-alg}

Given
\begin{equation}\label{eq-Ne}
N_e\geqslant 1
\end{equation}
extra proposal kernels $q_i:S\times S\rightarrow [0,1]$, $i=2,\ldots,N_e+1$,
the Delayed Rejection algorithm proceeds as follows:
\begin{enumerate}
\item for ($k=0$; $k < (N_{\mbox{chain}}-1)$; ++$k$) \{
\item $\quad$/* Perform $(k+1)$-th iteration in order to obtain $y^{(k+1)}$ */
\item $\quad$generate a candidate $c^{(1)}$ from $q_1(y^{(k)},\cdot)$;
\item $\quad$compute the acceptance ratio $\alpha=\alpha_1(y^{(k)},c^{(1)})$ (see \eqref{eq-alphas});
\item $\quad$draw $t$ from the uniform distribution on $[0,1]$;
\item $\quad$accept = false; $i=1$;
\item $\quad$if ($t\leqslant \alpha$) \{ $y^{(k+1)}=c^{(i)}$; accept = true; \}
\item $\quad$else while ((accept == false) \&\& ($i\leqslant N_e$)) \{
\item $\quad\quad$/* Extra stages trying extra candidates $c^{(2)},c^{(3)},\ldots,c^{(N_e+1)}$ (maximum) */
\item $\quad\quad$generate a candidate $c^{(i+1)}$ from $q_i(y^{(k)},c^{(1)},\ldots,c^{(i)})$;
\item $\quad\quad$compute the acceptance ratio $\alpha=\alpha_i(y^{(k)},c^{(1)},\ldots,c^{(i)})$ (see \eqref{eq-alphas});
\item $\quad\quad$draw $t$ from the uniform distribution on $[0,1]$;
\item $\quad\quad$if ($t\leqslant \alpha$) \{ $y^{(k+1)}=c^{(i)}$; accept = true; \}
\item $\quad\quad$$i\leftarrow i+1$;
\item $\quad$\} /* end while */
\item $\quad$if (accept == false) $y^{(k+1)}=y^{(k)}$;
\item \} /* end for */.
\end{enumerate}
So, if a rejection happens, the algorithm yet tries to find a suitable canditate under the same $(k+1)$-th iteration.
If $N_e=0$ then the DR algorithm becomes the MH algorithm of the previous subsection.

\subsection{The Adaptive Metropolis (AM) Algorithm}\label{subsc-mcmc-am-alg}
Given
\begin{equation}\label{eq-C0}
\mbox{a }n\times n\mbox{ covariance matrix }C_0,
\end{equation}
\begin{equation}\label{eq-sd}
\mbox{a real value }\eta >0,
\end{equation}
\begin{equation}\label{eq-epsilon}
\mbox{a real value }\epsilon>0,
\end{equation}
\begin{equation}\label{eq-t0}
\mbox{an integer }t_0>0,
\end{equation}
\begin{equation}\label{eq-n0}
\mbox{an integer }n_0>0,
\end{equation}
and, for $k > 0$ and any vectors $y^{(0)},y^{(1)},\ldots,y^{(k)}$, the expression
\begin{equation}\label{eq-emperical-cov}
\mbox{Cov}(y^{(0)},y^{(1)},\ldots,y^{(k)})\equiv,
\end{equation}
the Adaptive Metropolis algorithm proceeds as follows:
\begin{enumerate}
\item for ($k=0$; $k < (N_{\mbox{chain}}-1)$; ++$k$) \{
\item $\quad$/* Perform $(k+1)$-th iteration in order to obtain $y^{(k+1)}$ */
\item $\quad$generate a candidate $c$ from $q_1(y^{(k)},\cdot)$;
\item $\quad$compute the acceptance ratio $\alpha=\alpha_1(y^{(k)},c)$ (see \eqref{eq-alphas});
\item $\quad$draw $t$ from the uniform distribution on $[0,1]$;
\item $\quad$if ($t\leqslant \alpha$) $y^{(k+1)}=c$;
\item $\quad$else $y^{(k+1)}=y^{(k)}$;
\item \} /* end for */.
\end{enumerate}

\begin{sidewaystable}[h]
\begin{tabular}{|c||c|c||c|c||c|c|}
\hline
 Initial             & \multicolumn{2}{c||}{Metropolis-Hastings (``MH'')}                & \multicolumn{2}{c||}{Delayed Rejection (``DR'')}                         & \multicolumn{2}{c|}{Adaptive Metropolis (``AM'')}          \\
\cline{2-7}
 Position            & candidate               & next position                           & candidate              & next position                                   & candidate        & next position                           \\
(iteration)          &                         &                                         & (stage)                &                                                 &                  &                                         \\
\hline
\hline
$y^{(0)}$       & $c$ w/ $q$              & $y^{(0)}$ or $c$ w/ $\alpha_1$       & $c^{(1)}$ w/ $q_1$     & $y^{(0)}$ or $c^{(1)}$ w/ $\alpha_1$       & $c$ w/ $g_0$     & $y^{(0)}$ or $c$ w/ $\alpha_1$       \\
\cline{4-5}
                     &                         &                                         & $c^{(2)}$ w/ $q_2$     & $y^{(0)}$ or $c^{(2)}$ w/ $\alpha_2$       &                  &                                         \\
\cline{4-5}
                     &                         &                                         & $\vdots$               & $\vdots$                                        &                  &                                         \\
\cline{4-5}
                     &                         &                                         & \multicolumn{2}{l||}{until candidate accepted}                           &                  &                                         \\
                     &                         &                                         & \multicolumn{2}{l||}{or maximum stages achieved}                         &                  &                                         \\
\hline 
\hline
$y^{(1)}$       & $c$ w/ $q$              & $y^{(1)}$ or $c$ w/ $\alpha_1$       & $c^{(1)}$ w/ $q_1$     & $y^{(1)}$ or $c^{(1)}$ w/ $\alpha_1$       & $c$ w/ $g_0$     & $y^{(1)}$ or $c$ w/ $\alpha_1$       \\
\cline{4-5}
                     &                         &                                         & $c^{(2)}$ w/ $q_2$     & $y^{(1)}$ or $c^{(2)}$ w/ $\alpha_2$       &                  &                                         \\
\cline{4-5}
                     &                         &                                         & $\vdots$               & $\vdots$                                        &                  &                                         \\
\cline{4-5}
                     &                         &                                         & \multicolumn{2}{l||}{until candidate accepted}                           &                  &                                         \\
                     &                         &                                         & \multicolumn{2}{l||}{or maximum stages achieved}                         &                  &                                         \\
\hline
\hline
$\vdots$             & $\vdots$                & $\vdots$                                & $\vdots$               & $\vdots$                                        & $\vdots$         & $\vdots$                                \\
\hline
\hline
$y^{(t_0)}$     & $c$ w/ $q$              & $y^{(t_0)}$ or $c$ w/ $\alpha_1$     & $c^{(1)}$ w/ $q_1$     & $y^{(t_0)}$ or $c^{(1)}$ w/ $\alpha_1$     & $c$ w/ $g_1$     & $y^{(t_0)}$ or $c$ w/ $\alpha_1$     \\
\cline{4-5}
                     &                         &                                         & $\vdots$               & $\vdots$                                        &                  &                                         \\
\hline
\hline
$y^{(t_0+1)}$   & $c$ w/ $q$              & $y^{(t_0+1)}$ or $c$ w/ $\alpha_1$   & $c^{(2)}$ w/ $q_1$     & $y^{(t_0+1)}$ or $c^{(1)}$ w/ $\alpha_1$   & $c$ w/ $g_1$     & $y^{(t_0+1)}$ or $c$ w/ $\alpha_1$   \\
\cline{4-5}
                     &                         &                                         & $\vdots$               & $\vdots$                                        &                  &                                         \\
\hline
\hline
$\vdots$             & $\vdots$                & $\vdots$                                & $\vdots$               & $\vdots$                                        & $\vdots$         & $\vdots$                                \\
\hline
\hline
$y^{(t_0+n_0)}$ & $c$ w/ $q$              & $y^{(t_0+n_0)}$ or $c$ w/ $\alpha_1$ & $c^{(2)}$ w/ $q_1$     & $y^{(t_0+n_0)}$ or $c^{(1)}$ w/ $\alpha_1$ & $c$ w/ $g_2$     & $y^{(t_0+n_0)}$ or $c$ w/ $\alpha_1$ \\
\cline{4-5}
                     &                         &                                         & $\vdots$               & $\vdots$                                        &                  &                                         \\
\hline
\hline
$\vdots$             & $\vdots$                & $\vdots$                                & $\vdots$               & $\vdots$                                        & $\vdots$         & $\vdots$                                \\
\hline
\hline
$y^{(t_0+2n_0)}$& $c$ w/ $q$              & $y^{(t_0+2n_0)}$ or $c$ w/ $\alpha_1$& $c^{(2)}$ w/ $q_1$     & $y^{(t_0+2n_0)}$ or $c^{(1)}$ w/ $\alpha_1$& $c$ w/ $g_3$     & $y^{(t_0+2n_0)}$ or $c$ w/ $\alpha_1$\\
\cline{4-5}
                     &                         &                                         & $\vdots$               & $\vdots$                                        &                  &                                         \\
\hline
\hline
$\vdots$             & $\vdots$                & $\vdots$                                & $\vdots$               & $\vdots$                                        & $\vdots$         & $\vdots$                                \\
\hline
\end{tabular}
\caption{Overview of three algorithms for the generation of a {\it realization} of a Markov chain 
$\{y^{(0)},y^{(1)},\ldots\}$
: Metropolis-Hastings, Delayed Rejection and Adaptive Metropolis.
Detailed explanations are given in Section \ref{sc-mcmc-realization-of-a-markov-chain}.
}
\label{tab-dram}
\end{sidewaystable}

\subsection{The DRAM Algorithm}%\label{subsc-mcmc-dram-alg}
$~$\\

\section{How to Use DRAM}\label{sc-mcmc-how-to-use-dram}

In order to use the DRAM implementation of the PECOS Toolkit, one needs to take the following steps:
\begin{itemize}
\item {prepare your executable (let us call it ``myappl''):
\begin{itemize}
\item execute ``cd uq/appls/mcmc/'';
\item execute ``cp -R template myappl'';
\item execute ``cd myappl'';
\item change ``template'' and ``Template'' to ``myappl'' and ``Myappl'' in all files, including the file ``Makefile'';
\item code your prior function, if necessary (there is a default prior in the PECOS library);
\item code your likelihood function, which returns a vector of values, each value correspond to the likelihood of a specific output quantity, as explained in Section \ref{sc-intro-qoi}, page \pageref{sc-intro-qoi};
\item execute ``make myappl'';
\end{itemize}
}
\item prepare a file describing the input parameters; let us call it ``myappl.par''; all input parameters are assumed to be scalar r.v.'s; see Figure \ref{fig-dram-par-file-ex}; 
\item prepare an input file setting the algorithm options; let us call it ``myappl.inp''; see Figure \ref{fig-dram-input-file-ex} and Table \ref{tab-dram-map};
\item execute ``./myappl -i myappl.inp''.
\end{itemize}

\begin{figure}[h]
\begin{verbatim}
# This is the file of input parameters for "myappl".
# Tha name of this file must match the entry "uqParamSpace_inputFile" in
# the input file.
# Lines that begin with the character `#' are considered comment lines.
# Each line that is not a comment line is treated as representing an input
# parameter.
# From left to right, the entries in each input parameter line correspond to:
# --> parameter name (mandatory)
# --> initial value (mandatory)
# --> minimum value (optional; default value to "-inf")
# --> maximum value (optional; default value to "inf")
# --> expectation (optional; default value to "nan")
# --> variance (optional; default value to "inf")
Theta_0 0. -inf
# Comment lines can appear anywhere in the file.
Theta_1 0. -inf inf 0. inf
Theta_2 0.
Theta_3 0. -inf inf 0. inf
# The total number of input parameter lines (4 in this example) must match
# the entry "uqParamSpace_dim" in the input file.
\end{verbatim}
\caption{Example of a file of input parameters for the generation of a Markov Chain.
}
\label{fig-dram-par-file-ex}
\end{figure}

\begin{figure}[h]
\begin{verbatim}
###############################################
# UQ Parameter Space
###############################################
uqParamSpace_dim       = 4
uqParamSpace_inputFile = uqNormalEx.par

###############################################
# UQ Output Space
###############################################
uqOutputSpace_dim  = 1

###############################################
# UQ DRAM Markov Chain Generator
###############################################
uqDRAM_mh_sizesOfChains           = 5000
uqDRAM_mh_lrUpdateSigma2          = 0
uqDRAM_mh_lrSigma2Priors          = 1.
uqDRAM_mh_lrSigma2Accuracies      = 0.
uqDRAM_mh_lrNumbersOfObs          = 0
uqDRAM_dr_maxNumberOfExtraStages  = 0
uqDRAM_dr_scalesForExtraStages    = 5. 4. 3.
uqDRAM_am_initialNonAdaptInterval = 0
uqDRAM_am_adaptInterval           = 0
uqDRAM_am_sd                      = 1.44
uqDRAM_am_epsilon                 = 1.e-5
uqDRAM_mh_namesOfOutputFiles      = uqNormalExOutput.m
uqDRAM_mh_chainDisplayPeriod      = 500
\end{verbatim}
\caption{Example of an input file for the generation of a Markov Chain.
}
\label{fig-dram-input-file-ex}
\end{figure}

\begin{sidewaystable}
\begin{tabular}{|l|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Option Name}        & Explanation       & Default Value & \multicolumn{2}{c|}{Definition} \\
\cline{4-5}
                                         &                   &               & Equation    & Page              \\
\hline
\verb=uqParamSpace_dim=                  &                   &               &             &                   \\
\hline
\verb=uqParamSpace_inputFile=            &                   &               &             &                   \\
\hline
\verb=uqOutputSpace_dim=                 &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_sizesOfChains=           &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_lrUpdateSigma2=          &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_lrSigma2Priors=          &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_lrSigma2Accuracies=      &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_lrNumbersOfObs=          &                   &               &             &                   \\
\hline
\verb=uqDRAM_dr_maxNumberOfExtraStages=  &                   &               &             &                   \\
\hline
\verb=uqDRAM_dr_scalesForExtraStages=    &                   &               &             &                   \\
\hline
\verb=uqDRAM_am_initialNonAdaptInterval= &                   &               &             &                   \\
\hline
\verb=uqDRAM_am_adaptInterval=           &                   &               &             &                   \\
\hline
\verb=uqDRAM_am_sd=                      &                   &               &             &                   \\
\hline
\verb=uqDRAM_am_epsilon=                 &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_namesOfOutputFiles=      &                   &               &             &                   \\
\hline
\verb=uqDRAM_mh_chainDisplayPeriod=      &                   &               &             &                   \\
\hline
\end{tabular}
\caption{Mapping between DRAM options in the input file of Figure \ref{fig-dram-input-file-ex} and the mathematical terms explained in Sections \ref{sc-intro-qoi} and \ref{sc-mcmc-realization-of-a-markov-chain}.
}
\label{tab-dram-map}
\end{sidewaystable}

\section{Examples Provided}\label{sc-mcmc-dram-examples}

Three examples related to DRAM are provided: normal target distribution, chemical reactions and algae.
They are discussed in Subsections \ref{subsc-mcmc-dram-normal-ex}, \ref{subsc-mcmc-dram-himmel-ex} and \ref{subsc-mcmc-dram-algae-ex} respectively.

\subsection{Normal Target Distribution}\label{subsc-mcmc-dram-normal-ex}

To be explained in future versions of the documentation.

\subsection{Chemical Reactions}\label{subsc-mcmc-dram-himmel-ex}

To be explained in future versions of the documentation.

\subsection{Algae}\label{subsc-mcmc-dram-algae-ex}

\section{Planned Features for Next Releases}\label{sc-mcmc-planned-features}
With respect to Markov Chain Monte Carlo methods, the following features are planned for the next versions of the PECOS Toolkit for Predictive Engineering:
\begin{enumerate}
\item compute prediction with the chain of the algae example,
\item capability of running over parallel environments using Trilinos,
\item integration with the DAKOTA Toolkit,
\item chain convergence tests \cite{BrRo98},
\item capability of running over parallel environments using PETSc,
\item hyperprior models,
\item algorithms for multimodal distributions,
\item Gibbs sampler.
\end{enumerate}
