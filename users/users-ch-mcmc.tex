\chapter{Markov Chains}\label{ch-mcmc}
\thispagestyle{headings}
\markboth{Chapter \ref{ch-mcmc}: Markov Chains}{Chapter \ref{ch-mcmc}: Markov Chains}

In this chapter we review some concepts and results on Markov chains.
This chapter 
is based primarily on references \cite{Du05}, \cite{JaPr04} and \cite{So96}.
The subject of some sections can be found at \cite{KaSo05} and \cite{Ro96} as well.

\section{Basic Concepts}

Let $(\Omega,U,P)$ be a probability space and $Y:\Omega\rightarrow S$ be a $U$-measurable r.v.,
that is, a measurable map relative to the $\sigma$-algebras $U$ and $\mathcal{S}$,
as explained in Subsection \ref{subsc-intro-prelim-basic}, page \pageref{subsc-intro-prelim-basic}.

Now let $Y:\Omega\rightarrow\mathbb{R}$ be a $U$-measurable scalar r.v..
We will assume that the reader already knows the concept of the
integration of $Y$
w.r.t. a probability measure $M:\mathbb{R}\rightarrow [0,1]$
over a Borel set $B\in\mathfrak{B}(\mathbb{R})$ (see, e.g., \cite[Section A.4]{Du05} and \cite[Chapter 9]{JaPr04}).
When such integral exists (it might assume values $\pm\infty$), it will be denoted by
\begin{equation*}
\int_B y~dM(y).
\end{equation*}
When $M=P_Y$, the probability distribution of $Y$, we use the following notations:
\begin{equation*}
\int_B y~dP_Y(y) = 
\int_{Y^{-1}(B)}Y(\omega)dP(\omega).
\end{equation*}

Given a r.v. $\mathbf{Y}:\Omega\rightarrow\mathbb{R}^n$ and any arbitrarily fixed $B\in\mathfrak{B}(\mathbb{R}^n)$,
we define the r.v. 
\begin{equation*}
\mathbf{1}_{\mathbf{Y}\in B}:\Omega\rightarrow\{0,1\},
~\mathbf{1}_{\mathbf{Y}\in B}(\omega)=1\text{ if }\mathbf{Y}(\omega)\in B,
~\mathbf{1}_{\mathbf{Y}\in B}(\omega)=0\text{ otherwise}.
\end{equation*}

For a vector r.v. $Y:\Omega\rightarrow\mathbb{R}^n$ and $B\in\mathfrak{B}(\mathbb{R}^n)$,
the concepts presented so far are just applied componentwise.

\section{Expectations}

Let $\mathbf{Y}:\Omega\rightarrow \mathbb{R}^n$ be a $U$-measurable r.v. on the probability space $(\Omega,U,P)$.

The {\it expectation} $E_P[\mathbf{Y};A]\in\mathbb{R}^n$
of $\mathbf{Y}$
over an event $A\in U$
w.r.t. the probability measure $P:\Omega\rightarrow [0,1]$
is defined by
\begin{equation*}
E_P[\mathbf{Y};A] \equiv
\int_{A} \mathbf{Y}(\omega)~dP(\omega),
\end{equation*}
when such integral exists, including values $\pm\infty$.
We might simply write $E[\mathbf{Y};A]$.
When $A=\Omega$ we might simply write $E_P[\mathbf{Y}]$ or $E[\mathbf{Y}]$.

When appropriate for the clarification of expressions
we might eventually denote expectations with brakets $\{\cdot\}$ instead of using the $[\cdot]$ notation.

The {\it expectation} $E_M[\mathbf{Y};B]\in\mathbb{R}^n$
of $\mathbf{Y}$
over a Borel set $B\in\mathfrak{B}(\mathbb{R}^n)$
w.r.t. a probability measure $M:\mathbb{R}^n\rightarrow [0,1]$
is defined by
\begin{equation*}
E_M[\mathbf{Y};B] \equiv
\int_{B} \mathbf{y}~dM(\mathbf{y})
\end{equation*}
when such integral exists, including values $\pm\infty$.
When $B=\mathbb{R}^n$ we simply write
$E_M[\mathbf{Y}]$.

When $M=P_{\mathbf{Y}}$, the probability distribution of $\mathbf{Y}$, we write
\begin{equation*}
E_{P_{\mathbf{Y}}}[\mathbf{Y};B] \equiv
\int_{B} \mathbf{y}~dP_\mathbf{Y}(\mathbf{y}) =
\int_{\mathbf{Y}^{-1}(B)}\mathbf{Y}(\omega)dP(\omega) \equiv
E_P[\mathbf{Y};\mathbf{Y}^{-1}(B)]
\end{equation*}
or simply
$E[\mathbf{Y};B]$.

If $M=P_{\mathbf{Y}}$ and $B=\mathbb{R}^n$, then we use the following notations:
\begin{equation*}
E_{P_{\mathbf{Y}}}[\mathbf{Y}] \equiv
\int_{\mathbb{R}^n} \mathbf{y}~dP_\mathbf{Y}(\mathbf{y}) = 
\int_{\Omega}\mathbf{Y}(\omega)dP(\omega) \equiv
E_P[\mathbf{Y}],
\end{equation*}
or simply
$E[\mathbf{Y}]$.

\section{Integrable R.V.'s}

A r.v. $\mathbf{Y}$ is called {\it integrable} (w.r.t. the probability measure $P_{\mathbf{Y}}$) if and only if its expectation exists and is finite.
We will write $\mathcal{L}^1(\Omega,U,P)$, sometimes just $\mathcal{L}^1$ for short, to denote the vector space of all integrable random variables.
It is easy to check that:
\[
\begin{array}{rl}
(i)  & Y\in\mathcal{L}^1\text{ iff }|Y|\in\mathcal{L}^1,\text{ and}   \\
(ii) & Y_1=Y_2\text{ almost surely (a.s.) }\Rightarrow~E[Y_1]=E[Y_2]. \\
\end{array}
\]
For $1 < p < \infty$ we define $\mathcal{L}^p(\Omega,U,P)$ to be the space of r.v.'s $Y$ such that $|Y|^p\in\mathcal{L}^1$.

Since the a.s. equality is an equivalence relation between two r.v.'s,
we define $L^p(\Omega,U,P)$, $1\leqslant p < \infty$,
to be $\mathcal{L}^p$ module the ``a.s. equality'' equivalence relation,
and we might denote it $L^p$ for short. That is,
two elements of $\mathcal{L}^p$ that are a.s. equal are considered to be
representatives (``versions'') of the same element in $L^p$.

The space $L^2(\Omega,U,P)$ is a Hilbert space with inner product
\begin{equation*}
\langle \mathbf{Y},\mathbf{Z} \rangle = E[\mathbf{Y}\mathbf{Z}].
\end{equation*}
The space $L^2(\Omega,\sigma(\mathbf{Y}),P)$ is a (closed) Hilbert subspace of $L^2(\Omega,U,P)$.
The space $L^1(\Omega,\mathbf{Y},P)$ is a Banach space.
% with norm

For the case of a generic r.v. $\mathbf{Y}:\Omega\rightarrow S$,
let $h:S\rightarrow\mathbb{R}$ be a measurable map relative to $\mathcal{S}$ and $\mathfrak{B}(\mathbb{R})$, that is,
a r.v. on $(S,\mathcal{S},P_{\mathbf{Y}})$. See Figure \ref{fig-mcmc-hY-diagram}. One can then prove that \cite{JaPr04}
\begin{equation*}
h(\mathbf{Y})\in\mathcal{L}^1(\Omega,U,P)\text{ iff }h\in\mathcal{L}^1(S,\mathcal{S},P_{\mathbf{Y}}),
\end{equation*}
and
\begin{equation*}
h\text{ is positive or }h\in\mathcal{L}^1(S,\mathcal{S},P_{\mathbf{Y}})
\Rightarrow
E[h(\mathbf{Y})] = \int_S h(\mathbf{Y})P_{\mathbf{Y}}(d\mathbf{y}).
\end{equation*}
Moreover, if $S=\mathbb{R}^n$ then
\begin{equation*}
\mathbf{Y}\text{ has a probability density }\pi_{\mathbf{Y}}:\mathbb{R}^n\rightarrow [0,1]
\Rightarrow
E[h(\mathbf{Y})] = \int_{\mathbb{R}^n}h(\mathbf{y})\pi_{\mathbf{Y}}(\mathbf{y})~d\mathbf{y}.
\end{equation*}

\begin{figure}[h]
\[
\begin{CD}
\Omega   @>Y>> S           @>h>> \mathbb{R}              \\
\in      @.    \in         @.    \in                     \\
U        @.    \mathcal{S} @.    \mathfrak{B}(\mathbb{R})\\
@VP VV         @VVP_YV           @VVP_hV                 \\
[0,1]    @.    [0,1]       @.    [0,1]
\end{CD}
\]
\caption{The composition of a measurable map $h$ with a r.v. $\mathbf{Y}$,
resulting on a new r.v. $h(\mathbf{Y})$.
}
\label{fig-mcmc-hY-diagram}
\end{figure}

\section{Conditional Expectation}

Let $(\Omega,U,P)$ be a probability space,
$\mathcal{F}$ be a sub-$\sigma$-algebra of $U$,
and $Y:\Omega\rightarrow\mathbb{R}^n$ be an integrable r.v., that is, $\mathbf{Y}\in L^1(\Omega,U,P)$.
Note that we are {\it not} assuming that $Y$ is $\mathcal{F}$-measurable.
The {\it conditional expectation} of $\mathbf{Y}$ given $\mathcal{F}$,
denoted $E[\mathbf{Y}|\mathcal{F}]$,
is defined \cite{JaPr04} as the unique r.v. $E[\mathbf{Y}|\mathcal{F}]=\mathbf{X}:\Omega\rightarrow\mathbb{R}^n$ satisfying
\begin{equation}\label{eq-l1-cond-expec-wrt-algebra-A}
\mathbf{X}\in L^1(\Omega,\mathcal{F},P)
\end{equation}
and
\begin{equation}\label{eq-l1-cond-expec-wrt-algebra-B}
E_P[\mathbf{Y}\mathbf{Z};\Omega]\equiv E[\mathbf{Y}\mathbf{Z}] = E[\mathbf{X}\mathbf{Z}] \equiv E_P[\mathbf{X}\mathbf{Z};\Omega]
\quad
\forall\text{ bounded }\mathcal{F}\text{-measurable }\mathbf{Z}:\Omega\rightarrow\mathbb{R}^n.
\end{equation}
An alternative definition \cite{Du05} replaces \eqref{eq-l1-cond-expec-wrt-algebra-B} by
\begin{equation*}
E_P[\mathbf{Y};A]\equiv \int_A \mathbf{Y}(\omega)~dP(\omega) = \int_A \mathbf{X}(\omega)~dP(\omega)\equiv E_P[\mathbf{X};A] \quad\forall A\in\mathcal{F}.
\end{equation*}
For the case of $\mathbf{Y}\in L^2(\Omega,U,P)$ it can be shown \cite{JaPr04} that the
unique r.v. $\mathbf{X}:\Omega\rightarrow\mathbb{R}^n$ defined by \eqref{eq-l1-cond-expec-wrt-algebra-A}-\eqref{eq-l1-cond-expec-wrt-algebra-B} satisfies
\begin{equation}\label{eq-l2-cond-expec-wrt-algebra-A}
\mathbf{X}\in L^2(\Omega,\mathcal{F},P)
\end{equation}
and
\begin{equation}\label{eq-l2-cond-expec-wrt-algebra-B}
E[\mathbf{Y}\mathbf{Z}] = E[\mathbf{X}\mathbf{Z}]
\quad
\text{for all }\mathbf{Z}\in L^2(\Omega,\mathcal{F},P),
\end{equation}
that is, $\mathbf{X}$ can be interpreted as the projection of $\mathbf{Y}$ into the subspace $L^2(\Omega,\mathcal{F},P)$.

We define the conditional expectation $E[Y|X]$ of a r.v. $Y$ w.r.t. another r.v. $X$ by
\begin{equation*}
E[Y|X] = E[Y|\sigma(X)].
\end{equation*}

Given $\mathbf{Y}\in L^1(\Omega,U,P)$ and
a sub-$\sigma$-algebra $\mathcal{F}$,
we define,
for any given $B\in\mathfrak{B}(\mathbb{R}^n)$,
\begin{equation}\label{eq-cond-prob-wrt-algebra}
P_{\mathbf{Y}|\mathcal{F}}(B|\mathcal{F}) = E[\mathbf{1}_{\mathbf{Y}\in B}|\mathcal{F}].
\end{equation}

Some properties of conditional expectations are:
\begin{equation}\label{eq-cond-expec-know-Y}
Y\text{ is }\mathcal{F}\text{-measurable}
~\Rightarrow~
E[Y|\mathcal{F}]=Y,
\end{equation}
\begin{equation}\label{eq-cond-expec-know-nothing}
Y\text{ and }\mathcal{F}\text{ are independent (see }\eqref{eq-Y-F-independent}\text{)}
~\Rightarrow~
E[Y|\mathcal{F}]=E[Y],
\end{equation}
\begin{equation}\label{eq-cond-expec-independent-rvs}
Y\text{ and }X\text{ are independent r.v.'s}
~\Rightarrow~
E[Y|X]=E[Y],
\end{equation}
\begin{equation}\label{eq-cond-expec-smaller-wins}
\mathcal{F}_1\subset\mathcal{F}_2
~\Rightarrow~
E\{E[Y|\mathcal{F}_2]|\mathcal{F}_1\} = E[Y|\mathcal{F}_1]\text{ and }E\{E[Y|\mathcal{F}_1]|\mathcal{F}_2\} = E[Y|\mathcal{F}_1],
\end{equation}
\begin{equation}\label{eq-cond-expec-f-u}
\mathcal{F}\subset U\text{ and }E[Y|U]\text{ is }\mathcal{F}\text{-measurable}
~\Rightarrow~
E[Y|\mathcal{F}] = E[Y|U].
\end{equation}
and
\begin{equation}\label{eq-cond-expec-exists-g}
\left\{
\begin{array}{c}
\mathcal{F}=\sigma(X)\text{ for some r.v. }X:\Omega\rightarrow\mathbb{R}^m \\
~\Rightarrow~ \\
\exists~\mathfrak{B}(\mathbb{R}^m)\text{-measurable }g:\mathbb{R}^m\rightarrow\mathbb{R}^n\text{ such that }E[Y|\mathcal{F}] = g(X).
\end{array}
\right.
\end{equation}

We can interpret the properties just listed by grasping the intuitive idea of a $\sigma$-algebra $\mathcal{F}$ in the context of r.v.'s.
Let us say that the outcome $\omega$ of a performed random experiment is unknown. Instead, for each $A\in\mathcal{F}$ we are told if $\omega\in A$.
In this scenario, more sets in $\mathcal{F}$ means more information to us for determining $\omega$ and might mean more information to us for determining the value $\mathbf{Y}(\omega)$ of a r.v..
Result \eqref{eq-cond-expec-know-Y}, for instance, establishes that if $\mathbf{Y}$ is $\mathcal{F}$-measurable then the information in $\mathcal{F}$ is enough to determine the value $\mathbf{Y}(\omega)$,
although the information might not be enough to determine $\omega$ itself.
On the other extreme, result \eqref{eq-cond-expec-know-nothing} establishes that an independent $\sigma$-algebra is useless for the determination of $\mathbf{Y}(\omega)$.
As a particular case, the trivial $\sigma$-algebra provides no information.
Result $\eqref{eq-cond-expec-smaller-wins}$ establishes that the smaller $\sigma$-algebras ``wins''.

\section{Transition Probability}

Given a function $Q:S\times\mathcal{S}\rightarrow [0,1]$, let us define
\begin{equation*}
\text{for any }y\in S,\text{ the mapping }Q_y:\mathcal{S}\rightarrow [0,1]\text{ by }Q_y(B)=Q(y,B),
\end{equation*}
and
\begin{equation*}
\text{for any }B\in\mathcal{S},\text{ the mapping }Q_B:S\rightarrow [0,1]\text{ by }Q_B(y)=Q(y,B).
\end{equation*}
The function $Q:S\times\mathcal{S}\rightarrow [0,1]$ is then said to be a {\it transition probability} if
\begin{equation*}
\text{for each }y\in S,\text{ the mapping }Q_y:\mathcal{S}\rightarrow [0,1]\text{ is a probability measure on }(S,\mathcal{S})
\end{equation*}
and
\begin{equation*}
\text{for each }B\in\mathcal{S},\text{ the mapping }Q_B:S\rightarrow [0,1]\text{ is a measurable function}.
\end{equation*}
Intuitively, the value $Q(y,B)$ can be understood as
the probability that a random variable will transit/change from the current value $y$ to a value in the region $B$.
It is a concept related to Markov Chains, as we will see in the next section.

\section{Filtration, Adapted Sequence of R.V.'s and Markov Chains}

Let $(\Omega,U,P)$ be a probability space and
let $\{\mathbf{Y}_k\}_{k\geqslant 0}$ be a sequence of r.v.'s
respectively defined over the probability spaces $(\Omega,U_k,P)$ and
taking values on the same measurable space $(S,\mathcal{S})$.
We refer to $S$ as the state space.

A sequence $\{\mathcal{F}_k\}$ of increasing $\sigma$-algebras over $\Omega$ is called a {\it filtration}.
We say that $\{\mathbf{Y}_k\}$ is {\it adapted} to the filtration $\{\mathcal{F}_k\}$ if $\mathbf{Y}_k$ is $\mathcal{F}_k$-measurable for all $k$.
From now on we will assume $U=U_k$ and $\mathcal{F}_k\subseteq U$ for all $k$.

By definition, a sequence $\{\mathbf{Y}_k\}$ of r.v.'s is said to be a Markov Chain, or to possess the Markov property, w.r.t. a filtration $\{\mathcal{F}_k\}$,
if $\{\mathbf{Y}_k\}$ is adapted to $\{\mathcal{F}_k\}$ and
\begin{equation}\label{eq-markov-chain-general-def-1}
P_{\mathbf{Y}_{k+1}|\mathcal{F}_k}(B|\mathcal{F}_k) = P_{\mathbf{Y}_{k+1}}(B|\sigma(\mathbf{Y}_k))\quad\forall B\in\mathcal{S}.
\end{equation}
It should be noted that, according to $\eqref{eq-cond-prob-wrt-algebra}$, $\eqref{eq-markov-chain-general-def-1}$ is an equality between r.v.'s.
Intuitively, it establishes that given the present, represented by $\sigma(\mathbf{Y}_k)$,
the rest of the past is irrelevant for predicting $\mathbf{Y}_{k+1}$.

It can be proved \cite{Du05} that $\{\mathbf{Y}_k\}$ is a Markov Chain w.r.t. a filtration $\{\mathcal{F}_k\}$ iff
there exist transition probabilities $Q_k:S\times\mathcal{S}\rightarrow [0,1]$ such that
\begin{equation}\label{eq-markov-chain-general-def-2}
P_{\mathbf{Y}_{k+1}|\mathcal{F}_k}(B|\mathcal{F}_k) = \mathbf{Q}_k(\mathbf{Y}_k,B)\quad\forall B\in\mathcal{S},
\end{equation}
where, for any arbitrarily fixed $B\in\mathcal{S}$, $\mathbf{Q}_k(\mathbf{Y}_k,B):\Omega\rightarrow [0,1]$ is the r.v. defined by
\begin{equation*}
\left[\mathbf{Q}_k(\mathbf{Y}_k,B)\right](\omega) = Q_k(\mathbf{Y}_k(\omega),B).
\end{equation*}

If $S$ is countable, then \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-countable-def-1}
P_{\mathbf{Y}_{k+1}}(
\{\mathbf{y}_{k+1}\}
|
\{\mathbf{y}_k\},
\ldots,
\{\mathbf{y}_0\}
)
=
P_{\mathbf{Y}_{k+1}}(
\{\mathbf{y}_{k+1}\}
|
\{\mathbf{y}_k\}
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1,
\end{equation}
and
\begin{equation}\label{eq-markov-chain-countable-def-2}
P_{\mathbf{Y}_{k+1}}(
\{\mathbf{y}_{k+1}\}
|
\{\mathbf{y}_k\},
\ldots,
\{\mathbf{y}_0\}
)
=
Q_k(
\mathbf{y}_k,
\mathbf{y}_{k+1}
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1.
\end{equation}

If $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$, then \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-R-def-1}
P_{\mathbf{Y}_{k+1}}(
B_{k+1}
|
B_k,
\ldots,
B_0
)
=
P_{\mathbf{Y}_{k+1}}(
B_{k+1}
|
B_k
)
\quad\forall
B_i\in\mathcal{S},~0\leqslant i\leqslant k+1,
\end{equation}
and
\begin{equation}\label{eq-markov-chain-R-def-2}
P_{\mathbf{Y}_{k+1}}(
B_{k+1}|
B_k
\ldots,
B_0
)
=
\int_{B_k} Q_k(
\mathbf{y},B_{k+1}
)~d\mathbf{y}
\quad\forall
B_i\in\mathcal{S},~0\leqslant i\leqslant k+1.
\end{equation}

If $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$ and
the r.v.'s are absolutely continuous, i.e., their probability distributions can be expressed in terms of probability densities,
then \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-Rac-def-1}
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k,
\ldots,
\mathbf{y}_0
)
=
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1,
\end{equation}
and
\begin{equation}\label{eq-markov-chain-Rac-def-2}
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k,
\ldots,
\mathbf{y}_0
)~d\mathbf{y}
=
dQ_k(
\mathbf{y}_k,
\mathbf{y}_{k+1},
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1.
\end{equation}

It should be noted that in {\it all} equations
\eqref{eq-markov-chain-countable-def-1}
-
\eqref{eq-markov-chain-Rac-def-2}
$P_{\mathbf{Y}_{k+1}}(\cdot|\ldots)$ now means a real value
instead of a r.v. as in \eqref{eq-markov-chain-general-def-1} and \eqref{eq-markov-chain-general-def-2}.

\section{Transition Probability Kernel}

For the case $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$,
a function $K:S\times S\rightarrow [0,1]$ is called the kernel of
a transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$
if $Q$ can be expressed by
\begin{equation*}
Q(y,B) = \int_B {K}(y,z)~dz \quad\forall (y,z)\in S\times\mathcal{S}.
\end{equation*}
Clearly, a transition probability kernel needs to satisfy
\begin{equation}\label{eq-kernel-necessary-cond}
\int_S {K}(y,z)~dz=1.
\end{equation}

So, by definition of a kernel, 
if $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$ and
the r.v.'s are absolutely continuous,
then
\eqref{eq-markov-chain-Rac-def-2} can be replaced by
\begin{equation}\label{eq-markov-chain-Rac-def-3}
\pi_{\mathbf{Y}_{k+1}}(
\mathbf{y}_{k+1}
|
\mathbf{y}_k,
\ldots,
\mathbf{y}_0
)
=
K_k(
\mathbf{y}_k,
\mathbf{y}_{k+1},
)
\quad\forall
\mathbf{y}_i\in S,~0\leqslant i\leqslant k+1.
\end{equation}

In the context of finite state spaces, similarly as one can interpret the probability distribution as a ``probability density'',
the concept of transition probability is already enough, in the sense that it already plays the role of a transition probability kernel.
Nonetheless, for the case of finite state spaces we will replace \eqref{eq-markov-chain-countable-def-2} by
\begin{equation}\label{eq-markov-chain-finite-def-3}
P_{\mathbf{Y}_{k+1}}(
\{j\}
|
\{i\},
\ldots,
\{\mathbf{y}_0\}
)
=
K_k(i,j)
\quad\forall
i,j,\mathbf{y}_l\in S,~0\leqslant l\leqslant k-1.
\end{equation}

Let there be $N\times N$ values $K(i,j)\geqslant 0,~1\leqslant i,j\leqslant N$.
In order to form a transition probability kernel with such values, for the case of a finite state space $S$ of size $N$, it suffices that
\begin{equation}\label{eq-kernel-finite-suff-cond}
\sum_{j=1}^N K(i,j) = \sum_{j\in S}K(i,j) = 1.
\end{equation}
The correspondence between \eqref{eq-kernel-finite-suff-cond} and \eqref{eq-kernel-necessary-cond} is clear.

\section{Homogeneous Markov Chains}

A Markov Chain is said to be {\it homogeneous} if the transition probabilities do not depend on $k$.
From now on, unless stated otherwise,
we will assume that a Markov Chain is homogeneous with transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$.

\section{The Propagation of a Transition Probability}

Given a homogeneous Markov chain $\{\mathbf{Y}_k:\Omega\rightarrow S\}$ with a transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$,
and an integer $r>0$,
we define $Q^r:S\times\mathcal{S}\rightarrow [0,1]$ to be the function that describes the effects of $r$ propagation steps on the chain, that is,
\begin{equation*}%\label{eq-def-Qr-general}
P_{\mathbf{Y}_{k+r}|\sigma(\mathbf{Y}_k)} (B|\mathcal{F}_k) = \mathbf{Q}^r(\mathbf{Y}_k,B).
\end{equation*}

If $(S,\mathcal{S}=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n))$,
$\mathbf{Y}_k$ has probability density $\pi_{\mathbf{Y}_k}:S\rightarrow\mathbb{R}$, and
the transition probability has a kernel $K:S\times S\rightarrow\mathbb{R}$,
then we define $K^r:S\times\mathcal{S}\rightarrow\mathbb{R}$ to be the function such that
\begin{equation}\label{eq-def-Qr-Rac}
\pi_{\mathbf{Y}_{k+r}|\mathbf{y}_k} (\mathbf{y}_{k+r}|\mathbf{y}_k) = \mathbf{K}^r(\mathbf{y}_k,\mathbf{y}_{k+r}).
\end{equation}

The case of finite states $S$ will be analysed in the next subsection.

\section{Finite State Spaces}

In this subsection we will give a simple example exploring the concepts of transition probability kernel and homogeneity.
Let us assume a finite state space with $N>0$ possible elements.
Without loss of generality, we can define $S$ to be the set $\{1,2,\ldots,N\}$ and $\mathcal{S}$ to be the $\sigma$-algebra formed by all subsets of $S$.
Suppose also we are given:
{\renewcommand{\labelitemi}{}
\begin{itemize}
\item $(i)$ a r.v. $Y_0$ with known probability distribution $P_{Y_0}:\mathcal{S}\rightarrow [0,1]$ and
\item $(ii)$ a kernel $K:S\times S\rightarrow [0,1]$ in the form of a $N\times N$ matrix $K$ whose components $K_ij\equiv K(i,j)$ satisfy \eqref{eq-kernel-finite-suff-cond}.
\end{itemize}
}

So, the conditional probability of $Y_{k+1}$ w.r.t. to each possible value of $Y_k$:
\begin{equation*}
P_{Y_{k+1}|\{i\}}(\{j\}|\{i\})={K}_{ij}.
\end{equation*}
Let us then find an explicit formula for
\begin{equation*}
P_{Y_{k+1}} = P_{mY_{k+1}},
\end{equation*}
the term on the right meaning the marginal distribution of $Y_{k+1}$ when thinking in terms of a joint probability distribution between $Y_{k+1}$ and $Y_k$.
For easiness of notation, we will simply write
\begin{equation*}
P_{Y_{k+1}|i}(j|i)={K}_{ij}.
\end{equation*}
%
By remembering that
\begin{equation*}
P_{Y_{k+1}|i}(j|i) = \frac{P_{Y_{k+1}Y_k}(j,i)}{P_{mY_k}(i)} = \frac{P_{Y_{k+1}Y_k}(j,i)}{P_{Y_k}(i)}
\end{equation*}
and
\begin{equation*}
P_{Y_{k+1}}(j) = P_{mY_{k+1}}(j) = P_{Y_{k+1}Y_k}(j,S) = \sum_{i=1}^{N}P_{Y_{k+1}Y_k}(j,i),
\end{equation*}
we conclude
\begin{equation*}
P_{Y_{k+1}}(j) = \sum_{i=1}^{N}P_{Y_k}(i){K}_{ij}.
\end{equation*}
Clearly, $P_{Y_{k+1}}$ is a measure, since
\begin{equation*}
\sum_{j=1}^{N}P_{Y_{k+1}}(j) = \sum_{j=1}^{N}\sum_{i=1}^{N}P_{Y_k}(i){K}_{ij} = \sum_{i=1}^{N}P_{Y_k}(i)\sum_{j=1}^{N}{K}_{ij} = \sum_{i=1}^{N}P_{Y_k}(i) = 1.
\end{equation*}
If we represent $P_{Y_k}$ as a vector in $\mathbb{R}^N$ and its transpose by $P_{Y_k}^T$, then
\begin{equation}\label{eq-markov-recursive-relation-finite}
P_{Y_{k+1}}^T = P_{Y_k}^TK,\quad k\geqslant 0,
\end{equation}
\begin{equation}\label{eq-markov-Kk}
P_{Y_k}^T = P_{Y_0}^T{K}^k,\quad k\geqslant 0,
\end{equation}
and
\begin{equation}\label{eq-markov-Kr}
P_{Y_{k+r}}^T = P_{Y_k}^T{K}^r,\quad k,r\geqslant 0.
\end{equation}

It can be proved that the chain
\begin{equation*}
\{Y_0,Y_1,\ldots\}
\end{equation*}
constructed in this way is a homogeneous Markov Chain
w.r.t. the filtration $\{\mathcal{F}_k\}$ with $\mathcal{F}_k=\sigma(Y_0,Y_1,\ldots,Y_k)$ and
with transition probability kernerl given by the matrix $K$.

For any general Markov Chain, the probability measure $P_{X_0}$ is called the {\it initial distribution} of the chain.

For the case $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n)$, given
{\renewcommand{\labelitemi}{}
\begin{itemize}
\item $(i)$ a r.v. $Y_0$ with known probability density $\pi_{Y_0}:\mathcal{S}\rightarrow [0,1]$ and
\item $(ii)$ a transition probability kernel $K:S\times S\rightarrow [0,1]$,
\end{itemize}
}
we can similarly construct a Markov Chain as done for the case of a finite state space.
Equation \eqref{eq-markov-recursive-relation-finite} now becomes
\begin{equation}\label{eq-markov-recursive-relation-Rac}
\pi_{Y_{k+1}}(z) = \int_{S}\pi_{Y_k}(y)~K(y,z)~dy,\quad\forall z\in S,~k\geqslant 0.
\end{equation}

For the more general case of $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n)$, given
{\renewcommand{\labelitemi}{}
\begin{itemize}
\item $(i)$ a r.v. $Y_0$ with known probability distribution $P_{Y_0}:\mathcal{S}\rightarrow [0,1]$ and
\item $(ii)$ a transition probability $Q:S\times\mathcal{S}\rightarrow [0,1]$,
\end{itemize}
}
we can also similarly construct a Markov Chain, with
Equation \eqref{eq-markov-recursive-relation-finite} now becoming
\begin{equation}\label{eq-markov-recursive-relation-R}
P_{Y_{k+1}}(B) = \int_{y\in S}dP_{Y_k}(y)~Q(y,B),\quad\forall B\in\mathcal{S},~k\geqslant 0.
\end{equation}

\section{Stationary Distributions and Irreducible Aperiodic Chains}

For the case of finite state spaces,
a probability measure (i.e., distribution) $M:S\rightarrow [0,1]$ is said to be stationary w.r.t. a transition probability kernel $K$ if
\begin{equation*}
M = MK,
\end{equation*}
that is,
\begin{equation}\label{eq-stationary-finite-distribution}
M(j) = \sum_{i\in S}M(i)~K(i,j),\quad\forall j\in S.
\end{equation}

For the case of $(S,\mathcal{S})=(\mathbb{R}^n,\mathfrak{B}(\mathbb{R}^n)$,
a probability density $\pi_M:S\rightarrow [0,1]$ is said to be stationary w.r.t. a transition probability kernel $K$ if
\begin{equation}\label{eq-stationary-density}
\pi_M(z) = \int_{S}\pi_M(y)~K(y,z)~dy,\quad\forall z\in S.
\end{equation}

Simililarly to the correspondence between \eqref{eq-kernel-finite-suff-cond} and \eqref{eq-kernel-necessary-cond},
the correspondence between \eqref{eq-stationary-finite-distribution} and \eqref{eq-stationary-density} is clear.

For a general measurable space $(S,\mathcal{S})$,
a probability measure $M:S\rightarrow [0,1]$ is said to be stationary w.r.t. a transition probability $Q$ if
\begin{equation*}
M(B) = \int_{y\in S}dM(y)~Q(y,B),\quad\forall B\in\mathcal{S}.
\end{equation*}

\section{Limit Density of a Transition Probability Kernel}
$~$\\

\section{Ergodic Sequences}

To be explained in future versions of the documentation.

\section{Variance, Covariance and Covariance Matrix}

Although the concepts of variance and covariance are very important, we delayed their definitions in order to make it clear that Markov chains involve the concepts of probability distributions and expectation primarily.
Covariance matrices will be explicitly mentioned during the presentation of the Adaptive Metropolis algorithm in Subsection \ref{subsc-rmc-am-alg}.
